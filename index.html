<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization">
  <!-- 简化社交媒体元信息 -->
  <meta property="og:title" content="DualReal: Video Customization Framework"/>
  <meta property="og:description" content="Achieving identity-motion fusion through adaptive joint training"/>
  <meta property="og:image" content="<image-banner-social>">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="video generation, deep learning, identity preservation">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>DualReal</title>
  
  <!-- 简化样式表 -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <style>
    /* 参考网站基础样式 */
    body {
      font-family: 'Georgia', serif;
      line-height: 1.6;
      color: #333;
    }
    
    .section {
      padding: 4rem 1.5rem;
    }
    
    .title {
      font-weight: 600;
      margin-bottom: 1.5rem;
    }
    
    .paper-title {
      font-size: 2.2rem;
      line-height: 1.2;
      margin-bottom: 1.2rem;
    }

    .author-list {
      font-size: 1.1rem;
      margin: 1.5rem 0;
    }

    .content-block {
      max-width: 800px;
      margin: 0 auto;
    }

    .video-container {
      position: relative;
      padding-bottom: 56.25%;
      margin: 2rem 0;
    }

    .video-container iframe {
      position: absolute;
      width: 100%;
      height: 100%;
    }

    /* 修改图片容器样式 */
    .teaser-image {
        margin: 4rem auto 3rem;  /* 增加上下边距 */
        max-width: 1400px;      /* 增大最大宽度 */
        padding: 0 2rem;        /* 增加水平内边距 */
    }

    .teaser-image img {
        width: 100%;           /* 保持响应式宽度 */
        height: auto;          /* 保持原始比例 */
        border-radius: 6px;    /* 增大圆角半径 */
    }

    /* 图片说明文字调整 */
    .teaser-image p {
        font-size: 1.1rem;     /* 增大字体 */
        margin-top: 1.5rem;     /* 增加间距 */
    }

    /* 响应式调整 */
    @media screen and (max-width: 768px) {
        .teaser-image {
        margin: 3rem auto;
        padding: 0 1.5rem;
        }
        
        .teaser-image img {
        border-radius: 4px;
        }

        .teaser-image p {
        font-size: 1rem;
        padding: 0 1rem;
        }
    }

    @media screen and (min-width: 1600px) {
        .teaser-image {
        max-width: 90vw;     /* 在超大屏幕上使用视口单位 */
        }
    }

  </style>
</head>

<body>
  <!-- 简化Header部分 -->
  <section class="section">
    <div class="content-block">
      <h1 class="title paper-title has-text-centered">
        DualReal: Adaptive Joint Training for<br>Lossless Identity-Motion Fusion
      </h1>
      
      <div class="author-list has-text-centered">
        <span>
          <a href="mailto:wenc_k@mail.ustc.edu.cn" target="_blank">Wenchuan Wang</a><sup>1</sup>,
        </span>
        <span>
          <a href="mailto:huangmq@mail.ustc.edu.cn" target="_blank">Mengqi Huang</a><sup>1</sup>,
        </span>
        <span>
          <a href="mailto:tuyijing@mail.ustc.edu.cn" target="_blank">Yijing Tu</a><sup>1</sup>,
        </span>
        <span>
          <a href="mailto:zdmao@ustc.edu.cn" target="_blank">Zhendong Mao</a><sup>1*</sup>
        </span>
        <div class="institution mt-2">
          <sup>1</sup>University of Science and Technology of China
        </div>
      </div>

      <!-- 简化按钮 -->
      <div class="buttons is-centered are-medium">
        <a href="#paper" class="button is-rounded">
          <span class="icon"><i class="fas fa-file-pdf"></i></span>
          <span>Paper</span>
        </a>
        <a href="#code" class="button is-rounded is-dark">
          <span class="icon"><i class="fab fa-github"></i></span>
          <span>Code(coming soon)</span>
        </a>
      </div>

      <!-- 新增展示图区块 -->
      <div class="teaser-image" style="margin: 3rem auto 2rem; max-width: 1200px;">
        <img src="./img/result.png" 
             alt="DualReal Result Showcase"
             style="width: 100%; 
                    height: auto;
                    border-radius: 4px;">
        <p>
          Generated customization results of our proposed novel paradigm DualReal. Given subject images and motion videos, DualReal generates high-quality customized identity and motion simultaneously, without compromising the consistency of either dimension.
        </p>
      </div>
    </div>
  </section>

  <!-- 摘要部分 -->
  <section class="section" style="background: #f8f9fa;">
    <div class="content-block">
      <h2 class="title is-3 has-text-centered">Abstract</h2>
      <div class="content">
        <p>
            Customized text-to-video generation with pre-trained large-scale models has recently garnered significant attention through focusing on identity and motion consistency. Existing works typically follow the isolated customized paradigm, where the subject identity or motion dynamics are customized exclusively. However, this paradigm completely ignores the intrinsic mutual constraints and synergistic interdependencies between identity and motion, resulting in identity-motion conflicts throughout the generation process that systematically degrades. To address this, we introduce DualReal, a novel framework that, employs adaptive joint training to collaboratively construct interdependencies between dimensions. Specifically, DualReal is composed of two units: (1) Dual-aware Adaptation dynamically selects a training phase (i.e., identity or motion), learns the current information guided by the frozen dimension prior, and employs a regularization strategy to avoid knowledge leakage; (2) StageBlender Controller leverages the denoising stages and Diffusion Transformer depths to guide different dimensions with adaptive granularity, avoiding conflicts at various stages and ultimately achieving lossless fusion of identity and motion patterns. We constructed a more comprehensive evaluation benchmark than existing methods. The experimental results show that DualReal improves CLIP-I and DINO-I metrics by 21.7% and 31.8% on average, and achieves top performance on nearly all motion quality metrics.
        </p>
      </div>
    </div>
  </section>

  <!-- 方法图示 -->
  <section class="section">
    <div class="content-block">
      <h2 class="title is-3 has-text-centered">How does it work?</h2>
      <div class="has-text-centered">
        <img src="./img/pipeline.png" 
        alt="DualReal Pipeline"
        style="width: 100%; 
                height: auto;
                border-radius: 4px;">
      </div>
      <div class="content mt-4">
        <p> 
            <b>Training Paradigm:</b> At each training step, we first dynamically select the training phase Z(i.e., identity or motion) to determine the data processing path. The specific data undergoes noise injection and combines with the text embeddings. StageBlender Controller governs two-dimension adapters' contributions in Dual-Aware Block (DA-Block) through time-aware conditioning of current denoising step and fused feature representations. In DA-Block, the training-stage(Z) adapter learns the current information guided by the frozen dimension prior, and employs a regularization strategy to avoid dimensional knowledge leakage, achieving joint training. Both branches engage in residual connections with DiT outputs.  
        </p>
      </div>
      <div class="has-text-centered">
        <img src="./img/pipeline-2.png" 
        alt="DualReal Pipeline"
        style="width: 60%; 
                height: auto;
                border-radius: 4px;">
      </div>
      <div class="content mt-4">
        <p> 
            <b>StageBlender Controller</b> employs Adaptive LayerNorm mechanism  modulates text-visual feature based on timestep-conditional embeddings, then maps the feature to multiple groups after residual gated connections. These scaled weight are subsequently routed to their respective DA-Blocks for processing.
        </p>
      </div>
    </div>
  </section>

  <!-- 对比结果 -->
  <section class="section">
    <div class="content-block">
      <h2 class="title is-3 has-text-centered">Comparison Results</h2>
      <div class="has-text-centered">
        <img src="./img/mainresult.png" 
        alt="DualReal Pipeline"
        style="width: 100%; 
                height: auto;
                border-radius: 4px;">
      </div>
      <div class="content mt-4">
        <p> 
            <b>Qualitative comparison with existing methods.</b> Compared with other methods, DualReal achieves high identity consistency with coherent motion, demonstrating the advantage of joint training in balancing pattern conflicts.
        </p>
      </div>

      <div class="has-text-centered">
        <img src="./img/exp_png.png" 
        alt="DualReal Pipeline"
        style="width: 100%; 
                height: auto;
                border-radius: 4px;">
      </div>
      <div class="content mt-4">
        <p> 
          <b>Quantitative comparison of personalization video generation for customized subject and motion. "T.Cons" and "T.Flickering" denotes Temporal Consistency and Temporal Flickering, respectively. Compared with other methods, \textit{DualReal} achieved average improvements of <b>21.7\%</b> on CLIP-I and <b>31.8\%</b> on DINO-I, recorded the best results on three motion quality metrics (T.Cons, Motion Smoothness, and Temporal Flickering), and ranked second on CLIP-T. The motion datasets achieve an average Dynamic Degree of <b>12.02</b> and parenthetical values quantify the current method’s deviation from this benchmark to determine the intensity consistency of movement.
        </p>
      </div>
    </div>
  </section>

  <!-- 主展示视频 -->
  <section class="section">
    <div class="content-block">
      <h2 class="title is-3 has-text-centered">Video Demo</h2>
      <div class="video-container">
        <iframe src="https://www.youtube.com/embed/<video-youtube-id>" frameborder="0" allowfullscreen></iframe>
      </div>
    </div>
  </section>

  <!-- 引用部分 -->
  <section class="section" style="background: #f8f9fa;">
    <div class="content-block">
      <h2 class="title is-3 has-text-centered">Citation</h2>
      <pre style="background: #fff; padding: 1.5rem; border-radius: 4px;"><code>@article{dualreal2024,
  title={DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion},
  author={Wang, Wenchuan and Huang, Mengqi and Tu, Yijing and Mao, Zhendong},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2024}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
  
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
  
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- 基础脚本 -->
  <script defer src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>
</body>
</html>